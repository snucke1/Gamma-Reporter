name: Collect SPX Gamma Exposure (30-min)

on:
  # UTC schedule. Guarded by a market-open check in the job.
  schedule:
    - cron: '30 13 * * 1-5'      # 13:30 UTC ~ US cash open (EDT)
    - cron: '0,30 14-20 * * 1-5' # 14:00â€“20:30 UTC
    - cron: '0,30 21 * * 1-5'    # 21:00 UTC (covers EST overlap)
  workflow_dispatch:

permissions:
  contents: write   # allow this workflow to commit data to the repo

env:
  INDEX: SPX

jobs:
  collect:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # gamma_reporter imports these at module import time:
          pip install numpy pandas scipy matplotlib python-dateutil requests
          # For calendar and Parquet/CSV handling:
          pip install pandas_market_calendars pyarrow

      - name: Compute market-open (NYSE) safely
        id: nyse
        run: |
          python - << 'PY'
          import os, datetime
          import pandas_market_calendars as mcal

          now = datetime.datetime.now(datetime.timezone.utc)
          cal = mcal.get_calendar('XNYS')  # NYSE
          # Small window so weekends/holidays don't error out:
          start = (now - datetime.timedelta(days=1)).date()
          end   = (now + datetime.timedelta(days=1)).date()
          sched = cal.schedule(start_date=start, end_date=end)

          is_open = False
          if not sched.empty:
            try:
              is_open = cal.open_at_time(sched, now)
            except Exception:
              is_open = False

          print(f"UTC now: {now.isoformat()} | NYSE open: {is_open}")
          with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
            fh.write(f"is_open={'true' if is_open else 'false'}\n")
          PY

      - name: Collect & append to Parquet + CSV (no JSON)
        if: steps.nyse.outputs.is_open == 'true'
        env:
          INDEX: ${{ env.INDEX }}
        run: |
          mkdir -p data/intraday

          python - << 'PY'
          import os, sys, pathlib
          import pandas as pd
          import pyarrow as pa
          import pyarrow.parquet as pq

          # Import your collector
          sys.path.append('scripts')
          import collector

          index = os.getenv('INDEX', 'SPX')
          snap = collector.make_snapshot(index)  # dict with one row

          # ---- 1) Normalize missing numeric fields to float NaN ----
          for k in ["net_gex","gamma_ratio","zero_gamma","near_density"]:
              if snap.get(k, None) is None:
                  snap[k] = float('nan')

          # ---- 2) Make a 1-row DataFrame with fixed column order & dtypes ----
          cols = ["timestamp_utc","index","spot","net_gex","gamma_ratio","zero_gamma","near_density"]
          df = pd.DataFrame([[snap.get(c) for c in cols]], columns=cols).astype({
              "timestamp_utc": "string",
              "index": "string",
              "spot": "float64",
              "net_gex": "float64",
              "gamma_ratio": "float64",
              "zero_gamma": "float64",
              "near_density": "float64",
          })

          # Explicit Arrow schema keeps Parquet consistent
          schema = pa.schema([
              pa.field("timestamp_utc", pa.string()),
              pa.field("index",         pa.string()),
              pa.field("spot",          pa.float64()),
              pa.field("net_gex",       pa.float64()),
              pa.field("gamma_ratio",   pa.float64()),
              pa.field("zero_gamma",    pa.float64()),
              pa.field("near_density",  pa.float64()),
          ])
          new_tbl = pa.Table.from_pandas(df, schema=schema, preserve_index=False)

          p_out = pathlib.Path("data") / "intraday" / "gex_intraday.parquet"
          p_out.parent.mkdir(parents=True, exist_ok=True)

          if p_out.exists():
              old_tbl = pq.read_table(p_out)
              try:
                  # Preferred: cast new row to the existing file's schema
                  new_cast = new_tbl.cast(old_tbl.schema)
                  combined = pa.concat_tables([old_tbl, new_cast])
              except pa.lib.ArrowInvalid:
                  # If the old file has odd types, cast the old file to our schema, then append
                  old_cast = old_tbl.cast(schema)
                  combined = pa.concat_tables([old_cast, new_tbl])
              pq.write_table(combined, p_out)
          else:
              pq.write_table(new_tbl, p_out)

          # ----- CSV (Excel-friendly) with duplicate protection -----
          c_out = pathlib.Path("data") / "intraday" / "gex_intraday.csv"
          if c_out.exists():
              old = pd.read_csv(c_out)
              cat = pd.concat([old, df], ignore_index=True)
              cat = cat.drop_duplicates(subset=["timestamp_utc"], keep="last")
              cat.to_csv(c_out, index=False)
          else:
              df.to_csv(c_out, index=False)

          print("Appended 1 row to Parquet and CSV.")
          PY

      - name: Commit & push data
        if: steps.nyse.outputs.is_open == 'true'
        run: |
          git config user.name  "github-actions"
          git config user.email "github-actions@github.com"
          git add data/
          git commit -m "GEX snapshot $(date -u +'%Y-%m-%d %H:%MZ') (Parquet+CSV only)" || echo "No changes"
          git pull --rebase origin ${{ github.ref_name }} || true
          git push
